# import the library we will use
import piplite
await piplite.install('seaborn')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# get the data we want to use
from pyodide.http import pyfetch

async def download(url, filename):
    response = await pyfetch(url)
    if response.status == 200:
        with open(filename, "wb") as f:
            f.write(await response.bytes())

file_path= "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/automobileEDA.csv"
await download(file_path, "usedcars.csv")
file_name="usedcars.csv"

# check the downloaded data
df = pd.read_csv(file_name)
df.head()


# import the library scikit for modeling data
from sklearn.linear_model import LinearRegression

# create the single linear regression (SLR) object
lm = LinearRegression()
lm

# fit the data into the model
X = df[['highway-mpg']]
Y = df['price']
lm.fit(X,Y)

# make a predict value and stored it into a variable
Yhat=lm.predict(X)
Yhat[0:5] # show the array of the 5 result

#checking the value of the intercept and slope
lm.intercept_ #intercept
lm.coef_ #slope

# creating the multi linear regresion (MLR) model
Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']] #set the column data (x)
lm.fit(Z, df['price']) # fit the dataset into the linear variable

# import the visualization package: seaborn to check our model visualization
import seaborn as sns
%matplotlib inline

# checking the visualization of current data
width = 12 # width of the plot
height = 10 # the height of the plot
plt.figure(figsize=(width, height)) # set the size of the plot
sns.regplot(x="highway-mpg", y="price", data=df) #set the data into the plot
plt.ylim(0,) # set the start of value in the plot (y,x)


# checking the visualization of current data using residual plot to check what model we could use
# Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for the data
width = 12
height = 10
plt.figure(figsize=(width, height))
sns.residplot(x=df['highway-mpg'], y=df['price'])
plt.show()

#distribution plot to show the comparison of the model and the actual data
Y_hat = lm.predict(Z) # make the prediction data
plt.figure(figsize=(width, height)) #set the plot size

ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value") # set the actual data to red line
sns.distplot(Y_hat, hist=False, color="b", label="Fitted Values" , ax=ax1) # comparing the actual data with the predicted with blue line

plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()

# define the function of PolyPlot to create the plot for polynomial regresion visualization
def PlotPolly(model, independent_variable, dependent_variabble, Name):
    x_new = np.linspace(15, 55, 100)
    y_new = model(x_new)

    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')
    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')
    ax = plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig = plt.gcf()
    plt.xlabel(Name)
    plt.ylabel('Price of Cars')

    plt.show()
    plt.close()

# get the data first
x = df['highway-mpg']
y = df['price']

# Here we use a polynomial of the 3rd order (cubic), using the function polyfit, then use the function poly1d to display the polynomial function
f = np.polyfit(x, y, 3) #set the data into polynomial
p = np.poly1d(f) # set the data into model/formula to find the yhat
print(p) # show the model/formula

# call the function to show the plot from our model
PlotPolly(p, x, y, 'highway-mpg')
# p is the model/formula we set, x is the highway-mpg current data, y is the price current data,
#'highway-mpg' is the x name of the plot


# import the library to use the polynomial + pipelines
from sklearn.preprocessing import PolynomialFeatures

# set the variable pr as PolynomialFeatures with 2 degree
pr=PolynomialFeatures(degree=2)
pr

# get the data column from variable Z before and the save it to another variable
Z_pr=pr.fit_transform(Z)

# to check the shape of the data before transform
Z.shape # the result is (201,4) = 201 record data with 4 features

# to chek the shape of the data after transform
Z_pr.shape # the result is (201,15) = 201 record data with 15 features

# Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use StandardScaler as a step in our pipeline.
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# We create the pipeline by creating a list of tuples including the name of the model or estimator and its corresponding constructor.
Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]
# this means we normalize the value first using StandardScaler, and then convert it to polynomial, and then put the data into model LinearRegression

# We input the list as an argument to the pipeline constructor:
pipe=Pipeline(Input)
pipe

# First, we convert the data type Z to type float to avoid conversion warnings that may appear as a result of StandardScaler taking float inputs.
# Then, we can normalize the data, perform a transform and fit the model simultaneously.
Z = Z.astype(float)
pipe.fit(Z,y) #fit the data into the pipe (Z and y represent as we can see from the above code

